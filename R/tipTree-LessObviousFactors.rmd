---
title: "Final Project"
subtitle: "Data Exploration"
author: ""
date: "2019-Feb-25"
fontsize: 8 pt
output:
  #    beamer_presentation: #everything after hash "#" will be ignored 
pdf_document:
number_sections: false
fig_width: 6
fig_height: 4
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      #include = TRUE, 
                      eval = TRUE, 
                      
                      fig.width = 6, fig.height = 4,
                      results='hold',
                      warning = FALSE,
                      message=FALSE,
                      cache = TRUE,
                      digits = 3) 
```

```{r,echo=FALSE,results='hide', message=FALSE}
# Packages and Data Parser
#Note: Be sure plyr is loaded before dplyr
PackageList =c("plyr","dplyr","tidyverse","lubridate","censusapi","forcats","rpart","rpart.plot","tictoc","gbm","rattle")
NewPackages=PackageList[!(PackageList %in% 
                            installed.packages()[,"Package"])]
if(length(NewPackages)) install.packages(NewPackages,repos = "http://cran.us.r-project.org")
lapply(PackageList,require,character.only=TRUE)#array function

options(tibble.print_max = Inf, tibble.print_min = 20) 
set.seed(1) #Always set the seed for reproducibility
```

Load data and begin generating exploratory plots.

```{r,"Load Data"}
source("taxiData.R")
#Load records from taxi data source and store as a dataframe
data1m <- taxiData(2000000)
df <- data.frame(data1m$rawData)
rm(data1m) #Drop old data
gc()
df$dropoff_community_area<-as.factor(df$dropoff_community_area)
df$dropoff_centroid_location.coordinates<-as.character(df$dropoff_centroid_location.coordinates)
df$pickup_centroid_location.coordinates<-as.character(df$pickup_centroid_location.coordinates)
df$trip_total<-as.numeric(df$trip_total)
df$extras<-as.numeric(df$extras)
df$tolls<-as.numeric(df$tolls)
#Load census data by Chicago Community Area
ccaData <- read.csv("ChicagoCCAData.csv")
ccaData$CCA.Code<-as.factor(ccaData$CCA.Code)
ccaData$GEOG<-as.factor(ccaData$GEOG)
#Look up name of pickup CCA and add to df
df<- df %>% left_join(dplyr::select(ccaData,GEOG,CCA.Code),by=c("pickup_community_area"="CCA.Code"))
names(df)[names(df) == 'GEOG'] <- 'pickup_area'
#Look up name of dropoff CCA and add to df
df<- df %>% left_join(dplyr::select(ccaData,GEOG,CCA.Code),by=c("dropoff_community_area"="CCA.Code"))
names(df)[names(df) == 'GEOG'] <- 'dropoff_area'
#Look up population>16yo of pickup CCA and add to df
df<- df %>% left_join(dplyr::select(ccaData,POP_16OV,CCA.Code),by=c("pickup_community_area"="CCA.Code"))
names(df)[names(df) == 'POP_16OV'] <- 'Pop16Plus'
#Look up Labor Force of pickup CCA and add to df
df<- df %>% left_join(dplyr::select(ccaData,IN_LBFRC,CCA.Code),by=c("pickup_community_area"="CCA.Code"))
names(df)[names(df) == 'IN_LBFRC'] <- 'IN_LBFRC'
#Look up Unemp of pickup CCA and add to df
df<- df %>% left_join(dplyr::select(ccaData,UNEMP,CCA.Code),by=c("pickup_community_area"="CCA.Code"))
names(df)[names(df) == 'UNEMP'] <- 'UNEMP'
#Look up Unemp of pickup CCA and add to df
df<- df %>% left_join(dplyr::select(ccaData,MEDINC,CCA.Code),by=c("pickup_community_area"="CCA.Code"))
names(df)[names(df) == 'MEDINC'] <- 'MedIncome'
```
Clean data. For instance, "Chicago Elite Cab Corp. (Chicago Carriag" and "Chicago Elite Cab Corp." are clearly the same firm. We replace the former with the latter. 
We add factors for the Start Date (as opposed to date time), End Date, and Start Day of Week.
* In company, set "Chicago Elite Cab Corp. (Chicago Carriag" equal to "Chicago Elite Cab Corp."
* Add factor for Start Date (not date time)
* Add factor for End Date (not date time)
* Add factor for End Day of week
* Add factor for End month
* Add factor for End hour

```{r, "Add columns"}
df$company[df$company=="Chicago Elite Cab Corp. (Chicago Carriag"] <- "Chicago Elite Cab Corp."
df$startDate <- date(df$trip_start_timestamp)
df$endDate <- date(df$trip_end_timestamp)
df$endDay <- wday(df$trip_end_timestamp,label=TRUE)
df$endMonth <- month(df$trip_end_timestamp,label=TRUE)
df$endHour <- hour(str_replace(df$trip_end_timestamp,"T"," "))
#Replace <NA> with string "NA" which can be used for grouping
df$company <- fct_explicit_na(df$company,"NA") 
df$LaborPart <- ((df$IN_LBFRC-df$UNEMP)/df$IN_LBFRC)
```

Throw out all rows where NAs exist for dropoff/pickup location and other fields to be used in model. We will group all pickup areas originating fewer than 0.5% of all trip into 1 factor called "Other"

```{r, "Create slim dataset for modeling"}
#Calculate # trips by area and order, most to least. Pick out those with at least 0.5% of trips. 
topPickup <- df %>% drop_na(c("pickup_area")) %>% dplyr::group_by(pickup_area) %>% tally(sort=TRUE)
topPickup05Perc <- topPickup[topPickup$n>(.005*nrow(drop_na(df,c("pickup_area")))),]
#Remove unused levels
topPickup05Perc$pickup_area <- droplevels(topPickup05Perc$pickup_area)
#Add levels, including one for "Other"
df$pickup_area_group <- as.factor(ifelse(!df$pickup_area %in% topPickup05Perc$pickup_area,"Other",df$pickup_area))
levels(df$pickup_area_group)<-c(levels(topPickup05Perc$pickup_area),"Other")

#Do the same thing for company to reduce the number of factor levels
topCompany <- df %>% drop_na(c("company")) %>% dplyr::group_by(company) %>% tally(sort=TRUE)
topCompany05Perc <- topCompany[topCompany$n>(.001*nrow(drop_na(df,c("company")))),]

topCompany05Perc$company <- droplevels(topCompany05Perc$company)
#Add levels, including one for "Other"
df$company_group <- as.factor(ifelse(!df$company %in% topCompany05Perc$company,"Other",df$company))
levels(df$company_group)<-c(levels(topCompany05Perc$company),"Other")

#Filter on the following columns--drop all rows where NA encountered in any column
colFilter <- c("company_group","payment_type","tips","UNEMP","MedIncome","endDay","endMonth","endHour","LaborPart","pickup_area_group","fare","trip_total","trip_seconds")

dfSlim <- df[colFilter] %>% drop_na()
#Drop records where fare is below the min, likely bad
dfSlim <- dfSlim[dfSlim$trip_total>=3.25,]
#Drop records where duration =0
dfSlim <- dfSlim[dfSlim$trip_seconds > 0,]
dfSlim$dollarsPerHour<-dfSlim$trip_total/(dfSlim$trip_seconds/(60*60))
#Drop fare, tips, time
dfSlim <- dfSlim[,-which(names(dfSlim) %in% c("fare","trip_seconds","tips","trip_total"))]

cat("Filtering NA records left ", round(100*nrow(dfSlim)/nrow(df),1),"% of data. Dropping original dataframe. Leaves ", nrow(dfSlim)," total records.")
#rm(df)
#gc()
```

```{r}
#Drop any row containing NA in any column
dfTree <- dfSlim
dfTree <- dfTree[complete.cases(dfTree),]

nSample <- floor(0.75*nrow(dfTree))
train_ind <- sample(seq_len(nrow(dfTree)), size = nSample)

trainSample <- dfTree[train_ind,] #drop the final redundant dummy var for target_control
validateSample <- dfTree[-train_ind,]
```
# Regression

Let's measure RMSE using the same training and validation data sets for a linaer regression model.

```{r, "Logistic Regression - fare"}
phatT=list()
phatV=list()
phatT$logist = matrix(0.0,nrow(trainSample),1)
phatV$logist = matrix(0.0,nrow(validateSample),1)
#logistic regression, y=FALSE and model=FALSE to save memory
lgfit = glm(dollarsPerHour~.,trainSample,family="gaussian",y=FALSE,model=TRUE)

phatT$logist = predict(lgfit, trainSample, type="response")
phatV$logist = predict(lgfit, validateSample, type="response")

errorTrain = sqrt(mean((phatT$logist-trainSample$dollarsPerHour)^2))
errorValidate = sqrt(mean((phatV$logist-validateSample$dollarsPerHour)^2))


cat("Avg trip total per min: $",round(mean(dfSlim$dollarsPerHour),2))
cat("\nLinear Regression Training error: $",round(errorTrain,2))
cat("\nLinear Regression Validation error: $",round(errorValidate,2))
```
# Trees

One hypothesis is that there are different subsets of riders which we may be able to identify based upon:
* pickup location
* dropoff location
* pickup area demographic attributes
* time of ride
* day of ride

We will use tree-based models to model trip total per minute


We will construct a big tree to start.
```{r}
big_tree = rpart(dollarsPerHour~., data=trainSample, control=rpart.control(minsplit=50,cp=0.00001,xval=10))
nbig = length(unique(big_tree$where)) 

cat('size of big tree: ',nbig,'\n')

cptable = printcp(big_tree)
# this is the cp parameter with smallest cv-errror
(index_cp_min = which.min(cptable[,"xerror"]))
(cp_min = cptable[ index_cp_min, "CP" ])   

# one standard deviation rule 
# need to find first cp value for which the xerror is below horizontal line on the plot
(val_h = cptable[index_cp_min, "xerror"] + cptable[index_cp_min, "xstd"])
(index_cp_std = Position(function(x) x < val_h, cptable[, "xerror"]))
(cp_std = cptable[ index_cp_std, "CP" ])  
optimal.tree = prune(big_tree, cp=big_tree$cptable[6])
```
We then prune back this big tree for a range of different CP, calculating the misclassification rate and loss as a function of tree size for both our training and validation data.
```{r}
cpvec = big_tree$cptable[,"CP"]
ntree = length(cpvec)
tree_size = rep(0,ntree)

phatT$logist = matrix(0.0,nrow(trainSample),ntree)
phatV$logist = matrix(0.0,nrow(validateSample),ntree)

errorTrain=rep(0,ntree)
errorValidate=rep(0,ntree)

for(i in 1:ntree) {
  temptree = prune(big_tree,cp=cpvec[i])
  tree_size[i] = length(unique(temptree$where))
 
  phatT$logist = predict(temptree,newdata=trainSample)
  phatV$logist = predict(temptree,newdata=validateSample)
  
  errorTrain[i]=sqrt(mean((phatT$logist - trainSample$dollarsPerHour)^2))
  errorValidate[i]=sqrt(mean((phatV$logist - validateSample$dollarsPerHour)^2))
}

ggplot()+geom_line(aes(x=tree_size,y=errorValidate, color="validate"),size=1) +  geom_line(aes(x=tree_size,y=errorTrain,color="train"))+  scale_colour_manual(breaks = c("Validate"="blue", "Train"="green"),values = c("blue", "green")) + ggtitle("Tree Performance")+xlab("Tree Size")+ylab("RMSE")

plotcp(big_tree,upper="size")
fancyRpartPlot(big_tree)
fancyRpartPlot(optimal.tree)
##Print rules to file
sink("rules - Big Tree.txt")
rattle::asRules(big_tree,compact=FALSE)
sink()
sink("rules - optimal pruned tree.txt")
rattle::asRules(optimal.tree,compact=FALSE)
sink()
```
The error flatlines here--likely reflects the fact that we are excluding the major drivers of tip which are fare, distance, time. It might be better to look at tip or fare per minute.

## Trees with Boosting
When boosting we also want to try various combinations of model parameters (tree depth, number of trees, and shrinkage). For tree depth be try 1, 2, and 3. For number of trees we try 3 values from 500 to 1000 and for the shrinkage parameter we try values from .02 and .04.
```{r}
idv = c(1,3)
ntv = c(250)
shv = c(.005,.01,.02)
setboost = expand.grid(idv,ntv,shv)

colnames(setboost) = c("tdepth","ntree","shrink")
phatV$boost = matrix(0.0,nrow(validateSample),nrow(setboost))
phatT$boost = matrix(0.0,nrow(trainSample),nrow(setboost))
tdB=trainSample
vdB=validateSample;

for(i in 1:nrow(setboost)) {
tic()
##fit and predict
cat('Now i=',i ,'/',nrow(setboost), ', ')
fboost = gbm(dollarsPerHour~., data=tdB, distribution="gaussian",
  n.trees=setboost[i,2],
  interaction.depth=setboost[i,1],
  shrinkage=setboost[i,3])

phatT$boost[,i] = predict(fboost,newdata=tdB,n.trees=setboost[i,2],type="response")
phatV$boost[,i] = predict(fboost,newdata=vdB,n.trees=setboost[i,2],type="response")
toc()
}
```
Calculate train and validation error under
```{r}
errorTrain=rep(0,nrow(setboost))
errorValidate=rep(0,nrow(setboost))
cat("test")
for(i in 1:nrow(setboost)) {
  errorTrain[i]=sqrt(mean((phatT$boost[,i]-trainSample$dollarsPerHour)^2))
  cat('\nBoosting model ',i,' training error:',errorTrain[i])
  errorValidate[i]=sqrt(mean((phatV$boost[,i]-validateSample$dollarsPerHour)^2))
  cat('\nBoosting model ',i,' validation error:',errorValidate[i])
}

#errors = list()
#nmethod = ncol(phatV)
#for(i in 1:nmethod) {
#  nrun = ncol(phatV[[i]])
#  lvec = rep(0,nrun)
#  for(j in 1:nrun) lvec[j] = sqrt(mean((phatV$boost[,i]-validateSample$tips)^2))
#  lossL[[i]]=lvec; names(lossL)[i] = names(phatV)[i]
#}
#lossv = unlist(lossL)

#par(mfrow=c(1,1))
#plot(lossv, ylab="loss on validation", type="n")
#nloss=0
#for(i in 1:nmethod) {
#ii = nloss + 1:ncol(phatV[[i]])
#points(ii,lossv[ii],col=i,pch=17)
#nloss = nloss + ncol(phatV[[i]])
#}
```
