---
title: "Final Project"
subtitle: "Data Exploration"
author: ""
date: "2019-Feb-25"
fontsize: 8 pt
output:
  #    beamer_presentation: #everything after hash "#" will be ignored 
pdf_document:
number_sections: false
fig_width: 6
fig_height: 4
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      #include = TRUE, 
                      eval = TRUE, 
                      
                      fig.width = 6, fig.height = 4,
                      results='hold',
                      warning = FALSE,
                      message=FALSE,
                      cache = TRUE,
                      digits = 3) 
```

```{r,echo=FALSE,results='hide', message=FALSE}
# Packages and Data Parser
#Note: Be sure plyr is loaded before dplyr
PackageList =c("plyr","dplyr","tidyverse","lubridate","censusapi","forcats","rpart","rpart.plot","tictoc","gbm","rattle","data.table","glmnet")
NewPackages=PackageList[!(PackageList %in% 
                            installed.packages()[,"Package"])]
if(length(NewPackages)) install.packages(NewPackages,repos = "http://cran.us.r-project.org")
lapply(PackageList,require,character.only=TRUE)#array function

options(tibble.print_max = Inf, tibble.print_min = 20) 
set.seed(1) #Always set the seed for reproducibility
```
#Define helper functions
```{r}
# deviance loss function
# y should be 0/1
# phat are probabilities obtain by our algorithm
# wht shrinks probs in phat towards .5 --- this helps avoid numerical problems donâ€™t use log(0)!
lossf = function(y,phat,wht=0.0000001) {
if(is.factor(y)) y = as.numeric(y)-1
phat = (1-wht)*phat + wht*.5 #For numerical reason, we can not do log(0)
py = ifelse(y==1, phat, 1-phat)
return(-2*sum(log(py)))
}

# confusion matrix
# y should be 0/1
# phat are probabilities obtain by our algorithm
# thr is the cut off value - everything above thr is classified as 1
getConfusionMatrix = function(y,phat,thr=0.5) {
if(is.factor(y)) y = as.numeric(y)-1
yhat = ifelse(phat > thr, 1, 0)
tb = table(predictions = yhat,
actual = y)
rownames(tb) = c("predict_0", "predict_1")
return(tb)
}

# deviance loss function
# y should be 0/1
# phat are probabilities obtain by our algorithm
# thr is the cut off value - everything above thr is classified as 1
lossMR = function(y,phat,thr=0.5) {
if(is.factor(y)) y = as.numeric(y)-1
yhat = ifelse(phat > thr, 1, 0)
return(1 - mean(yhat == y))
}
```


Load data and begin generating exploratory plots.

```{r,"Load Data"}
#Load data from dataSummary cleanup file
source("dataSummary.R")
```
We perform some additional factor aggregation to make it easier when using trees

We will group all pickup/dropoff areas with fewer than 0.5% of all trip into 1 factor called "Other"

```{r, "Create slim dataset for modeling"}
#Calculate # trips by area and order, most to least. Pick out those with at least 0.5% of trips. 
topPickup <- dfClean2 %>% drop_na(c("pickup_area")) %>% dplyr::group_by(pickup_area) %>% tally(sort=TRUE)
topPickup05Perc <- topPickup[topPickup$n>(.005*nrow(drop_na(dfClean2,c("pickup_area")))),]
#Remove unused levels
topPickup05Perc$pickup_area <- droplevels(topPickup05Perc$pickup_area)
#Add levels, including one for "Other"
dfClean2$pickup_area_group <- as.factor(ifelse(!dfClean2$pickup_area %in% topPickup05Perc$pickup_area,"Other",dfClean2$pickup_area))
levels(dfClean2$pickup_area_group)<-c(levels(topPickup05Perc$pickup_area),"Other")
#Do the same for dropoffs
topDropoff <- dfClean2 %>% drop_na(c("dropoff_area")) %>% dplyr::group_by(dropoff_area) %>% tally(sort=TRUE)
topDropoff05Perc <- topDropoff[topDropoff$n>(.005*nrow(drop_na(dfClean2,c("dropoff_area")))),]
#Remove unused levels
topDropoff05Perc$dropoff_area <- droplevels(topDropoff05Perc$dropoff_area)
#Add levels, including one for "Other"
dfClean2$dropoff_area_group <- as.factor(ifelse(!dfClean2$dropoff_area %in% topDropoff05Perc$dropoff_area,"Other",dfClean2$dropoff_area))
levels(dfClean2$dropoff_area_group)<-c(levels(topDropoff05Perc$dropoff_area),"Other")

#Filter on the following columns--drop all rows where NA encountered in any column
colFilter <- c("company","payment_type","dropoff_area_group","pickup_area_group","startDay","startMonth","trip_total","trip_seconds","trip_miles","MedIncomePickupF","MedIncomeDropoffF","PercentUnempPickupF","PercentUnempDropoffF","tipOutcome")

dfSlim <- dfClean2[colFilter] 


#A few columns that won't enter the model
#toDrop <- c("tips")
#dfSlim <- dfSlim[,-which(names(dfSlim) %in% toDrop)]

cat("Filtering NA records left ", round(100*nrow(dfSlim)/nrow(df),1),"% of data. Dropping original dataframe. Leaves ", nrow(dfSlim)," total records.")
rm(df)
rm(dfClean)
gc()
```

```{r}
#Drop any row containing NA in any column
dfTree <- dfSlim[1:1000000,]
dfTest <- dfSlim[(dim(dfTree)[1]+1):(dim(dfTree)[1]+500000),]
nSample <- floor(0.75*nrow(dfTree))
train_ind <- sample(seq_len(nrow(dfTree)), size = nSample)

trainSample <- dfTree[train_ind,] #drop the final redundant dummy var for target_control
validateSample <- dfTree[-train_ind,]
```
# Regression

Let's measure RMSE using the same training and validation data sets for a linaer regression model. The first question we want to ask is what the baseline rate of tipping is across the entire dataset.
```{r}
cat("The rate of tipping within the dataset is: ", round(sum(dfSlim$tipOutcome)/nrow(dfSlim)*100,2),"%")
```

The first approach to investigate is a logistic regression for binomial classification:
```{r, "Logistic Regression - fare"}
phatT=list()
phatV=list()
#First logistic regression for binomial classification
lgfit = glm(tipOutcome~., trainSample, family=binomial(link = "logit"))

tempT = predict(lgfit, trainSample, type="response")
tempV = predict(lgfit, validateSample, type="response")

phatT$logit = matrix(tempT,ncol=1)
phatV$logit = matrix(tempV,ncol=1)

getConfusionMatrix(trainSample$tipOutcome, phatT[[1]][,1], 0.5)
cat("Training missclassification rate = ", round(100*lossMR(trainSample$tipOutcome, phatT[[1]][,1], 0.5),2), "%\n")
getConfusionMatrix(validateSample$tipOutcome, phatV[[1]][,1], 0.5)
cat("Validation missclassification rate = ", round(100*lossMR(validateSample$tipOutcome, phatV[[1]][,1], 0.5),2), "%\n")

```

The logistic model gives a relatively low misclassification rate but we are given the relative sensitivity of the output to each model factor level, a large number of co-variates. Next we would like to investigate the use of a lasso for variable reduction. This will reduce the dimensionality of the problem such that the final model only includes the most significant drivers of the tip outcome. These factors that remain will tell us the most significant indicators of whether or not a given trip will result in a tip.
```{r}
vars_name <- trainSample[,-which(names(trainSample) %in% c("tipOutcome"))] %>% 
  select_if(function(col) is.factor(col) | is.numeric(col)) %>% 
  colnames() %>% 
  str_c(collapse = "+") 
model_string <- paste("tipOutcome  ~",vars_name )

phatT$lasso = matrix(0.0,nrow(trainSample),1)
phatV$lasso = matrix(0.0,nrow(validateSample),1)

x_train <- model.matrix(as.formula(model_string), trainSample)
x_validate <- model.matrix(as.formula(model_string), validateSample)

lasso_cv <- cv.glmnet(x=x_train,y = as.matrix(trainSample$tipOutcome), family = "binomial", alpha=1, standardize=TRUE, nfolds=10)

##Plot important variables
c<-coef(lasso_cv,s='lambda.1se',exact=TRUE)
inds<-which(c!=0)
variables<-row.names(c)[inds]
betas<-c[inds]
cbind(variables,round(betas,6))

#View a plot of the lasso cross validation curve as a function of lambda
plot(lasso_cv)
#Show coefficents (all)
lasso=predict(lasso_cv,type="coefficients")

#Calculate probability of tip
phatT$lasso <- predict(lasso_cv,x_train,s=lasso_cv$lambda.1se, type="response")
phatV$lasso <- predict(lasso_cv,x_validate,s=lasso_cv$lambda.1se, type="response")

#Calculatemisclassifciation error
#Threshold for classifying as 1

getConfusionMatrix(trainSample$tipOutcome, phatT[[2]][,1], 0.5)
cat("Training missclassification rate = ", round(100*lossMR(trainSample$tipOutcome, phatT[[2]][,1], 0.5),2), "%\n")
getConfusionMatrix(validateSample$tipOutcome, phatV[[2]][,1], 0.5)
cat("Validation missclassification rate = ", round(100*lossMR(validateSample$tipOutcome, phatV[[2]][,1], 0.5),2), "%\n")

lossL = list()
phatL=phatT
nmethod = length(phatL)
for(i in 1:nmethod) {
nrun = ncol(phatL[[i]])
lvec = rep(0,nrun)
for(j in 1:nrun) lvec[j] = lossf(validateSample$tipOutcome, phatL[[i]][,j])
lossL[[i]]=lvec; names(lossL)[i] = names(phatL)[i]
}
lossv = unlist(lossL)
plot(lossv, ylab="loss on Test", type="n")
nloss=0
for(i in 1:nmethod) {
ii = nloss + 1:ncol(phatL[[i]])
points(ii,lossv[ii],col=i,pch=17)
nloss = nloss + ncol(phatL[[i]])
}
legend("topright",legend=names(phatL),col=1:nmethod,pch=rep(17,nmethod))


```

```{r, "Lasso Tips"}
library(dplyr)

vars_name <- trainSample[,-which(names(trainSample) %in% c("tipOutcome",))] %>% 
  select_if(function(col) is.factor(col) | is.numeric(col)) %>% 
  colnames() %>% 
  str_c(collapse = "+") 
model_string <- paste("tips  ~",vars_name )

x_train <- model.matrix(as.formula(model_string), trainSample)

lasso_cv <- cv.glmnet(x=x_train,y = as.matrix(trainSample$tips), family = "binomial", alpha=1, standardize=TRUE, nfolds=10)
plot(lasso_cv)
```

#########
# Trees

We will now use trees to identify characteristics of rides with average earnings rates with more than 1 sd above average. The mean and standard deviation of total fare are given by:
```{r}
#Filter on the following columns--drop all rows where NA encountered in any column
colFilter <- c("company","payment_type","dropoff_area_group","pickup_area_group","startDay","startMonth","trip_total","trip_seconds","trip_miles","MedIncomePickupF","MedIncomeDropoffF","PercentUnempPickupF","PercentUnempDropoffF","tipOutcome","hourlyRate")

dfSlim <- dfClean2[colFilter]
```
Plot box plot of hour fare


```{r}
boxplot(hourlyRate~pickup_area_group, data=dfSlim[1:1000000,],ylim=c(0,400),)
```


```{r}
meanFare <- mean(3600*dfSlim$trip_total/dfSlim$trip_seconds)
sdFare <- sd(3600*dfSlim$trip_total/dfSlim$trip_seconds)
cat("Average earning rate: ", mean(3600*dfSlim$trip_total/dfSlim$trip_seconds) )
```
We will add a column called hourlyOutcome which will include a binomial indicator to indicate whether the hourlyRate for a ride is greater than the meanHourlyRate by at least 1 stdev.

```{r}
dfSlim$hourlyOutcome=ifelse(dfSlim$hourlyRate>(meanFare+sdFare),1,0)
colFilter <- c("company","payment_type","dropoff_area_group","pickup_area_group","startDay","startMonth","trip_total","trip_seconds","trip_miles","MedIncomePickupF","MedIncomeDropoffF","PercentUnempPickupF","PercentUnempDropoffF","hourlyOutcome")

#Filter further
dfSlim <- dfSlim[colFilter]

dfTree <- dfSlim[1:1000000,]
dfTest <- dfSlim[(dim(dfTree)[1]+1):(dim(dfTree)[1]+500000),]
nSample <- floor(0.75*nrow(dfTree))
train_ind <- sample(seq_len(nrow(dfTree)), size = nSample)

trainSample <- dfTree[train_ind,] #drop the final redundant dummy var for target_control
validateSample <- dfTree[-train_ind,]
```


```{r}
big_tree = rpart(hourlyOutcome~., data=trainSample, control=rpart.control(minsplit=50,cp=0.00001,xval=10))
nbig = length(unique(big_tree$where)) 

cat('size of big tree: ',nbig,'\n')

cptable = printcp(big_tree)
# this is the cp parameter with smallest cv-errror
(index_cp_min = which.min(cptable[,"xerror"]))
(cp_min = cptable[ index_cp_min, "CP" ])   

# one standard deviation rule 
# need to find first cp value for which the xerror is below horizontal line on the plot
(val_h = cptable[index_cp_min, "xerror"] + cptable[index_cp_min, "xstd"])
(index_cp_std = Position(function(x) x < val_h, cptable[, "xerror"]))
(cp_std = cptable[ index_cp_std, "CP" ])  
optimal.tree = prune(big_tree, cp=big_tree$cptable[6])
```
We then prune back this big tree for a range of different CP, calculating the misclassification rate and loss as a function of tree size for both our training and validation data.
```{r}
cpvec = big_tree$cptable[,"CP"]
ntree = length(cpvec)
tree_size = rep(0,ntree)

phatT$tree = matrix(0.0,nrow(trainSample),ntree)
phatV$tree = matrix(0.0,nrow(validateSample),ntree)

errorTrain=rep(0,ntree)
errorValidate=rep(0,ntree)

for(i in 1:ntree) {
  temptree = prune(big_tree,cp=cpvec[i])
  tree_size[i] = length(unique(temptree$where))
 
  phatT$tree = predict(temptree,newdata=trainSample)
  phatV$tree = predict(temptree,newdata=validateSample)
  
  errorTrain[i]=sqrt(mean((phatT$logist - trainSample$dollarsPerHour)^2))
  errorValidate[i]=sqrt(mean((phatV$logist - validateSample$dollarsPerHour)^2))
}



#Calculatemisclassifciation error
#Threshold for classifying as 1

getConfusionMatrix(trainSample$hourlyOutcome, phatT$tree, 0.5)
cat("Training missclassification rate = ", round(100*lossMR(trainSample$hourlyOutcome, phatT$tree, 0.5),2), "%\n")
getConfusionMatrix(validateSample$hourlyOutcome, phatV$tree, 0.5)
cat("Validation missclassification rate = ", round(100*lossMR(validateSample$hourlyOutcome, phatV$tree, 0.5),2), "%\n")





ggplot()+geom_line(aes(x=tree_size,y=errorValidate, color="validate"),size=1) +  geom_line(aes(x=tree_size,y=errorTrain,color="train"))+  scale_colour_manual(breaks = c("Validate"="blue", "Train"="green"),values = c("blue", "green")) + ggtitle("Tree Performance")+xlab("Tree Size")+ylab("RMSE")

plotcp(big_tree,upper="size")
fancyRpartPlot(big_tree)
fancyRpartPlot(optimal.tree)
##Print rules to file
sink("rules - Big Tree.txt")
rattle::asRules(big_tree,compact=FALSE)
sink()
sink("rules - optimal pruned tree.txt")
rattle::asRules(optimal.tree,compact=FALSE)
sink()
```
The error flatlines here--likely reflects the fact that we are excluding the major drivers of tip which are fare, distance, time. It might be better to look at tip or fare per minute.

## Trees with Boosting
When boosting we also want to try various combinations of model parameters (tree depth, number of trees, and shrinkage). For tree depth be try 1, 2, and 3. For number of trees we try 3 values from 500 to 1000 and for the shrinkage parameter we try values from .02 and .04.
```{r}
idv = c(1,3)
ntv = c(250)
shv = c(.005,.01,.02)
setboost = expand.grid(idv,ntv,shv)

colnames(setboost) = c("tdepth","ntree","shrink")
phatV$boost = matrix(0.0,nrow(validateSample),nrow(setboost))
phatT$boost = matrix(0.0,nrow(trainSample),nrow(setboost))
tdB=trainSample
vdB=validateSample;

for(i in 1:nrow(setboost)) {
tic()
##fit and predict
cat('Now i=',i ,'/',nrow(setboost), ', ')
fboost = gbm(dollarsPerHour~., data=tdB, distribution="gaussian",
  n.trees=setboost[i,2],
  interaction.depth=setboost[i,1],
  shrinkage=setboost[i,3])

phatT$boost[,i] = predict(fboost,newdata=tdB,n.trees=setboost[i,2],type="response")
phatV$boost[,i] = predict(fboost,newdata=vdB,n.trees=setboost[i,2],type="response")
toc()
}
```
Calculate train and validation error under
```{r}
errorTrain=rep(0,nrow(setboost))
errorValidate=rep(0,nrow(setboost))
cat("test")
for(i in 1:nrow(setboost)) {
  errorTrain[i]=sqrt(mean((phatT$boost[,i]-trainSample$dollarsPerHour)^2))
  cat('\nBoosting model ',i,' training error:',errorTrain[i])
  errorValidate[i]=sqrt(mean((phatV$boost[,i]-validateSample$dollarsPerHour)^2))
  cat('\nBoosting model ',i,' validation error:',errorValidate[i])
}

#errors = list()
#nmethod = ncol(phatV)
#for(i in 1:nmethod) {
#  nrun = ncol(phatV[[i]])
#  lvec = rep(0,nrun)
#  for(j in 1:nrun) lvec[j] = sqrt(mean((phatV$boost[,i]-validateSample$tips)^2))
#  lossL[[i]]=lvec; names(lossL)[i] = names(phatV)[i]
#}
#lossv = unlist(lossL)

#par(mfrow=c(1,1))
#plot(lossv, ylab="loss on validation", type="n")
#nloss=0
#for(i in 1:nmethod) {
#ii = nloss + 1:ncol(phatV[[i]])
#points(ii,lossv[ii],col=i,pch=17)
#nloss = nloss + ncol(phatV[[i]])
#}
```
