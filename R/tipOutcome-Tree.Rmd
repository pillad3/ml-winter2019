---
title: "Final Project"
subtitle: "Data Exploration"
author: ""
date: "2019-Feb-25"
fontsize: 8 pt
output:
  #    beamer_presentation: #everything after hash "#" will be ignored 
pdf_document:
number_sections: false
fig_width: 6
fig_height: 4
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      #include = TRUE, 
                      eval = TRUE, 
                      
                      fig.width = 6, fig.height = 4,
                      results='hold',
                      warning = FALSE,
                      message=FALSE,
                      cache = TRUE,
                      digits = 3) 
```

```{r,echo=FALSE,results='hide', message=FALSE}
# Packages and Data Parser
#Note: Be sure plyr is loaded before dplyr
PackageList =c("plyr","dplyr","tidyverse","lubridate","censusapi","forcats","rpart","rpart.plot","tictoc","gbm","rattle","data.table","glmnet")
NewPackages=PackageList[!(PackageList %in% 
                            installed.packages()[,"Package"])]
if(length(NewPackages)) install.packages(NewPackages,repos = "http://cran.us.r-project.org")
lapply(PackageList,require,character.only=TRUE)#array function

options(tibble.print_max = Inf, tibble.print_min = 20) 
set.seed(1) #Always set the seed for reproducibility
```

Load data and begin generating exploratory plots.

```{r,"Load Data"}
#Load data from dataSummary cleanup file
source("dataSummary.R")
```
We perform some additional factor aggregation to make it easier when using trees

We will group all pickup/dropoff areas with fewer than 0.5% of all trip into 1 factor called "Other"

```{r, "Create slim dataset for modeling"}
#Calculate # trips by area and order, most to least. Pick out those with at least 0.5% of trips. 
topPickup <- dfClean2 %>% drop_na(c("pickup_area")) %>% dplyr::group_by(pickup_area) %>% tally(sort=TRUE)
topPickup05Perc <- topPickup[topPickup$n>(.005*nrow(drop_na(dfClean2,c("pickup_area")))),]
#Remove unused levels
topPickup05Perc$pickup_area <- droplevels(topPickup05Perc$pickup_area)
#Add levels, including one for "Other"
dfClean2$pickup_area_group <- as.factor(ifelse(!dfClean2$pickup_area %in% topPickup05Perc$pickup_area,"Other",dfClean2$pickup_area))
levels(dfClean2$pickup_area_group)<-c(levels(topPickup05Perc$pickup_area),"Other")
#Do the same for dropoffs
topDropoff <- dfClean2 %>% drop_na(c("dropoff_area")) %>% dplyr::group_by(dropoff_area) %>% tally(sort=TRUE)
topDropoff05Perc <- topDropoff[topDropoff$n>(.005*nrow(drop_na(dfClean2,c("dropoff_area")))),]
#Remove unused levels
topDropoff05Perc$dropoff_area <- droplevels(topDropoff05Perc$dropoff_area)
#Add levels, including one for "Other"
dfClean2$dropoff_area_group <- as.factor(ifelse(!dfClean2$dropoff_area %in% topDropoff05Perc$dropoff_area,"Other",dfClean2$dropoff_area))
levels(dfClean2$dropoff_area_group)<-c(levels(topDropoff05Perc$dropoff_area),"Other")

#Filter on the following columns--drop all rows where NA encountered in any column
colFilter <- c("company","payment_type","dropoff_area_group","pickup_area_group","startDay","startMonth","trip_total","trip_seconds","trip_miles","MedIncomePickupF","MedIncomeDropoffF","PercentUnempPickupF","PercentUnempDropoffF","tipOutcome")

dfSlim <- dfClean2[colFilter] 


#A few columns that won't enter the model
#toDrop <- c("tips")
#dfSlim <- dfSlim[,-which(names(dfSlim) %in% toDrop)]

cat("Filtering NA records left ", round(100*nrow(dfSlim)/nrow(df),1),"% of data. Dropping original dataframe. Leaves ", nrow(dfSlim)," total records.")
rm(df)
rm(dfClean)
gc()
```

```{r}
#Drop any row containing NA in any column

dfTree <- dfSlim[1:1000000,]

nSample <- floor(0.75*nrow(dfTree))
train_ind <- sample(seq_len(nrow(dfTree)), size = nSample)

trainSample <- dfTree[train_ind,] #drop the final redundant dummy var for target_control
validateSample <- dfTree[-train_ind,]
```
# Regression

Let's measure RMSE using the same training and validation data sets for a linaer regression model.

```{r, "Logistic Regression - fare"}
phatT=list()
phatV=list()

vars_name <- trainSample[,-which(names(trainSample) %in% c("tipOutcome"))] %>% 
  select_if(function(col) is.factor(col) | is.numeric(col)) %>% 
  colnames() %>% 
  str_c(collapse = "+") 
model_string <- paste("tipOutcome  ~",vars_name )

phatT$logist = matrix(0.0,nrow(trainSample),1)
phatV$logist = matrix(0.0,nrow(validateSample),1)

x_train <- model.matrix(as.formula(model_string), trainSample)
x_validate <- model.matrix(as.formula(model_string), validateSample)

lasso_cv <- cv.glmnet(x=x_train,y = as.matrix(trainSample$tipOutcome), family = "binomial", alpha=1, standardize=TRUE, nfolds=10)

##Plot important variables
c<-coef(lasso_cv,s='lambda.1se',exact=TRUE)
inds<-which(c!=0)
variables<-row.names(c)[inds]
betas<-c[inds]
cbind(variables,round(betas,6))

#View a plot of the lasso cross validation curve as a function of lambda
plot(lasso_cv)
#Show coefficents (all)
predict(lasso_cv,type="coefficients")

#Calculate probability of tip
phatT$lasso <- predict(lasso_cv,x_train,s=lasso_cv$lambda.1se, type="response")
phatV$lasso <- predict(lasso_cv,x_validate,s=lasso_cv$lambda.1se, type="response")

#Calculatemisclassifciation error
#Threshold for classifying as 1
thresh<-0.5
phatT$lasso <- ifelse(phatT$lasso>thresh,1,0)
phatV$lasso <- ifelse(phatV$lasso>thresh,1,0)

plot(lasso)
plot(lasso, xvar = "lambda", label = TRUE)

#Calculate misclassification error

errorTrain = sqrt(mean((phatT$lasso-trainSample$tipOutcome)^2))
errorValidate = sqrt(mean((phatV$lasso-validateSample$tipOutcome)^2))
misclassTrain = sum(ifelse(phatT$lasso==trainSample$tipOutcome,0,1))/nrow(trainSample)
misclassValidate = sum(ifelse(phatV$lasso==validateSample$tipOutcome,0,1))/nrow(trainSample)

cat("\nLasso Regression Training Misclass Error: ",round(misclassTrain,4))
cat("\nLasso Regression Validation Misclass Error: ",round(misclassValidate,4))
cat("\nLasso Regression Training RSSE: ",round(errorTrain,4))
cat("\nLasso Regression Validation RMSE: ",round(errorValidate,4))
```

```{r, "Lasso Tips"}
library(dplyr)

vars_name <- trainSample[,-which(names(trainSample) %in% c("tipOutcome",))] %>% 
  select_if(function(col) is.factor(col) | is.numeric(col)) %>% 
  colnames() %>% 
  str_c(collapse = "+") 
model_string <- paste("tips  ~",vars_name )

x_train <- model.matrix(as.formula(model_string), trainSample)

lasso_cv <- cv.glmnet(x=x_train,y = as.matrix(trainSample$tips), family = "binomial", alpha=1, standardize=TRUE, nfolds=10)
plot(lasso_cv)
```


lasso <- glmnet(x=x_train,y = as.matrix(trainSample$tips), family = "binomial", alpha=1, standardize=TRUE,intercept = TRUE)


c<-coef(lasso_cv,s='lambda.1se',exact=TRUE)
inds<-which(c!=0)
variables<-row.names(c)[inds]
betas<-c[inds]
cbind(variables,round(betas,6))

plot(lasso)
plot(lasso, xvar = "lambda", label = TRUE)

```



# Trees

One hypothesis is that there are different subsets of riders which we may be able to identify based upon:
* pickup location
* dropoff location
* pickup area demographic attributes
* time of ride
* day of ride

We will use tree-based models to model trip total per minute


We will construct a big tree to start.
```{r}
big_tree = rpart(tipOutcome~., data=trainSample[1:subsetTrain,], control=rpart.control(minsplit=50,cp=0.00001,xval=10))
nbig = length(unique(big_tree$where)) 

cat('size of big tree: ',nbig,'\n')

cptable = printcp(big_tree)
# this is the cp parameter with smallest cv-errror
(index_cp_min = which.min(cptable[,"xerror"]))
(cp_min = cptable[ index_cp_min, "CP" ])   

# one standard deviation rule 
# need to find first cp value for which the xerror is below horizontal line on the plot
(val_h = cptable[index_cp_min, "xerror"] + cptable[index_cp_min, "xstd"])
(index_cp_std = Position(function(x) x < val_h, cptable[, "xerror"]))
(cp_std = cptable[ index_cp_std, "CP" ])  
optimal.tree = prune(big_tree, cp=big_tree$cptable[6])
```
We then prune back this big tree for a range of different CP, calculating the misclassification rate and loss as a function of tree size for both our training and validation data.
```{r}
cpvec = big_tree$cptable[,"CP"]
ntree = length(cpvec)
tree_size = rep(0,ntree)

phatT$logist = matrix(0.0,nrow(trainSample),ntree)
phatV$logist = matrix(0.0,nrow(validateSample),ntree)

errorTrain=rep(0,ntree)
errorValidate=rep(0,ntree)

for(i in 1:ntree) {
  temptree = prune(big_tree,cp=cpvec[i])
  tree_size[i] = length(unique(temptree$where))
 
  phatT$logist = predict(temptree,newdata=trainSample)
  phatV$logist = predict(temptree,newdata=validateSample)
  
  errorTrain[i]=sqrt(mean((phatT$logist - trainSample$dollarsPerHour)^2))
  errorValidate[i]=sqrt(mean((phatV$logist - validateSample$dollarsPerHour)^2))
}

ggplot()+geom_line(aes(x=tree_size,y=errorValidate, color="validate"),size=1) +  geom_line(aes(x=tree_size,y=errorTrain,color="train"))+  scale_colour_manual(breaks = c("Validate"="blue", "Train"="green"),values = c("blue", "green")) + ggtitle("Tree Performance")+xlab("Tree Size")+ylab("RMSE")

plotcp(big_tree,upper="size")
fancyRpartPlot(big_tree)
fancyRpartPlot(optimal.tree)
##Print rules to file
sink("rules - Big Tree.txt")
rattle::asRules(big_tree,compact=FALSE)
sink()
sink("rules - optimal pruned tree.txt")
rattle::asRules(optimal.tree,compact=FALSE)
sink()
```
The error flatlines here--likely reflects the fact that we are excluding the major drivers of tip which are fare, distance, time. It might be better to look at tip or fare per minute.

## Trees with Boosting
When boosting we also want to try various combinations of model parameters (tree depth, number of trees, and shrinkage). For tree depth be try 1, 2, and 3. For number of trees we try 3 values from 500 to 1000 and for the shrinkage parameter we try values from .02 and .04.
```{r}
idv = c(1,3)
ntv = c(250)
shv = c(.005,.01,.02)
setboost = expand.grid(idv,ntv,shv)

colnames(setboost) = c("tdepth","ntree","shrink")
phatV$boost = matrix(0.0,nrow(validateSample),nrow(setboost))
phatT$boost = matrix(0.0,nrow(trainSample),nrow(setboost))
tdB=trainSample
vdB=validateSample;

for(i in 1:nrow(setboost)) {
tic()
##fit and predict
cat('Now i=',i ,'/',nrow(setboost), ', ')
fboost = gbm(dollarsPerHour~., data=tdB, distribution="gaussian",
  n.trees=setboost[i,2],
  interaction.depth=setboost[i,1],
  shrinkage=setboost[i,3])

phatT$boost[,i] = predict(fboost,newdata=tdB,n.trees=setboost[i,2],type="response")
phatV$boost[,i] = predict(fboost,newdata=vdB,n.trees=setboost[i,2],type="response")
toc()
}
```
Calculate train and validation error under
```{r}
errorTrain=rep(0,nrow(setboost))
errorValidate=rep(0,nrow(setboost))
cat("test")
for(i in 1:nrow(setboost)) {
  errorTrain[i]=sqrt(mean((phatT$boost[,i]-trainSample$dollarsPerHour)^2))
  cat('\nBoosting model ',i,' training error:',errorTrain[i])
  errorValidate[i]=sqrt(mean((phatV$boost[,i]-validateSample$dollarsPerHour)^2))
  cat('\nBoosting model ',i,' validation error:',errorValidate[i])
}

#errors = list()
#nmethod = ncol(phatV)
#for(i in 1:nmethod) {
#  nrun = ncol(phatV[[i]])
#  lvec = rep(0,nrun)
#  for(j in 1:nrun) lvec[j] = sqrt(mean((phatV$boost[,i]-validateSample$tips)^2))
#  lossL[[i]]=lvec; names(lossL)[i] = names(phatV)[i]
#}
#lossv = unlist(lossL)

#par(mfrow=c(1,1))
#plot(lossv, ylab="loss on validation", type="n")
#nloss=0
#for(i in 1:nmethod) {
#ii = nloss + 1:ncol(phatV[[i]])
#points(ii,lossv[ii],col=i,pch=17)
#nloss = nloss + ncol(phatV[[i]])
#}
```
