---
title: "Final Project"
subtitle: "Data Exploration"
author: ""
date: "2019-Feb-25"
fontsize: 8 pt
output:
  #    beamer_presentation: #everything after hash "#" will be ignored 
pdf_document:
number_sections: false
fig_width: 6
fig_height: 4
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      #include = TRUE, 
                      eval = TRUE, 
                      
                      fig.width = 6, fig.height = 4,
                      results='hold',
                      warning = FALSE,
                      message=FALSE,
                      cache = TRUE,
                      digits = 3) 
```

```{r,echo=FALSE,results='hide', message=FALSE}
# Packages and Data Parser
#Note: Be sure plyr is loaded before dplyr
PackageList =c("plyr","dplyr","tidyverse","lubridate","censusapi","forcats","rpart","rpart.plot")
NewPackages=PackageList[!(PackageList %in% 
                            installed.packages()[,"Package"])]
if(length(NewPackages)) install.packages(NewPackages,repos = "http://cran.us.r-project.org")
lapply(PackageList,require,character.only=TRUE)#array function

options(tibble.print_max = Inf, tibble.print_min = 20) 
set.seed(1) #Always set the seed for reproducibility
```

Load data and begin generating exploratory plots.

```{r,"Load Data"}
source("taxiData.R")
#Load records from taxi data source and store as a dataframe
data1m <- taxiData(500000)
df <- data.frame(data1m$rawData)
rm(data1m) #Drop old data
gc()
df$dropoff_community_area<-as.factor(df$dropoff_community_area)
df$dropoff_centroid_location.coordinates<-as.character(df$dropoff_centroid_location.coordinates)
df$pickup_centroid_location.coordinates<-as.character(df$pickup_centroid_location.coordinates)
df$trip_total<-as.numeric(df$trip_total)
df$extras<-as.numeric(df$extras)
df$tolls<-as.numeric(df$tolls)
#Load census data by Chicago Community Area
ccaData <- read.csv("ChicagoCCAData.csv")
ccaData$CCA.Code<-as.factor(ccaData$CCA.Code)
ccaData$GEOG<-as.factor(ccaData$GEOG)
#Look up name of pickup CCA and add to df
df<- df %>% left_join(dplyr::select(ccaData,GEOG,CCA.Code),by=c("pickup_community_area"="CCA.Code"))
names(df)[names(df) == 'GEOG'] <- 'pickup_area'
#Look up name of dropoff CCA and add to df
df<- df %>% left_join(dplyr::select(ccaData,GEOG,CCA.Code),by=c("dropoff_community_area"="CCA.Code"))
names(df)[names(df) == 'GEOG'] <- 'dropoff_area'
#Look up population>16yo of pickup CCA and add to df
df<- df %>% left_join(dplyr::select(ccaData,POP_16OV,CCA.Code),by=c("pickup_community_area"="CCA.Code"))
names(df)[names(df) == 'POP_16OV'] <- 'Pop16Plus'
#Look up Labor Force of pickup CCA and add to df
df<- df %>% left_join(dplyr::select(ccaData,IN_LBFRC,CCA.Code),by=c("pickup_community_area"="CCA.Code"))
names(df)[names(df) == 'IN_LBFRC'] <- 'IN_LBFRC'
#Look up Unemp of pickup CCA and add to df
df<- df %>% left_join(dplyr::select(ccaData,UNEMP,CCA.Code),by=c("pickup_community_area"="CCA.Code"))
names(df)[names(df) == 'UNEMP'] <- 'UNEMP'
#Look up Unemp of pickup CCA and add to df
df<- df %>% left_join(dplyr::select(ccaData,MEDINC,CCA.Code),by=c("pickup_community_area"="CCA.Code"))
names(df)[names(df) == 'MEDINC'] <- 'MedIncome'
```
Clean data. For instance, "Chicago Elite Cab Corp. (Chicago Carriag" and "Chicago Elite Cab Corp." are clearly the same firm. We replace the former with the latter. 
We add factors for the Start Date (as opposed to date time), End Date, and Start Day of Week.
* In company, set "Chicago Elite Cab Corp. (Chicago Carriag" equal to "Chicago Elite Cab Corp."
* Add factor for Start Date (not date time)
* Add factor for End Date (not date time)
* Add factor for End Day of week
* Add factor for End month
* Add factor for End hour

```{r, "Add columns"}
df$company[df$company=="Chicago Elite Cab Corp. (Chicago Carriag"] <- "Chicago Elite Cab Corp."
df$startDate <- date(df$trip_start_timestamp)
df$endDate <- date(df$trip_end_timestamp)
df$endDay <- wday(df$trip_end_timestamp,label=TRUE)
df$endMonth <- month(df$trip_end_timestamp,label=TRUE)
df$endHour <- hour(str_replace(df$trip_end_timestamp,"T"," "))
#Replace <NA> with string "NA" which can be used for grouping
df$company <- fct_explicit_na(df$company,"NA") 
df$LaborPart <- ((df$IN_LBFRC-df$UNEMP)/df$IN_LBFRC)
```

Throw out all rows where NAs exist for dropoff/pickup location and other fields to be used in model. We will group all pickup areas originating fewer than 0.5% of all trip into 1 factor called "Other"

```{r, "Create slim dataset for modeling"}
#Calculate # trips by area and order, most to least. Pick out those with at least 0.5% of trips.
topPickup <- df %>% drop_na(c("pickup_area")) %>% dplyr::group_by(pickup_area) %>% tally(sort=TRUE)
topPickup05Perc <- topPickup[topPickup$n>(.005*nrow(drop_na(df,c("pickup_area")))),]
#Remove unused levels
topPickup05Perc$pickup_area <- droplevels(topPickup05Perc$pickup_area)
#Add levels, including one for "Other"
df$pickup_area_group <- as.factor(ifelse(!df$pickup_area %in% topPickup05Perc$pickup_area,"Other",df$pickup_area))
levels(df$pickup_area_group)<-c(levels(topPickup05Perc$pickup_area),"Other")

#Filter on the following columns--drop all rows where NA encountered in any column
colFilter <- c("pickup_area_group","fare","endHour","endMonth","endDay","trip_seconds","Pop16Plus","MedIncome","LaborPart")
dfSlim <- df[colFilter] %>% drop_na()
dfSlim <- dfSlim[dfSlim$fare>=2.25,]
cat("Filtering NA records left ", round(100*nrow(dfSlim)/nrow(df),1),"% of data. Dropping original dataframe.")
#rm(df)
#gc()
```

# Trees

One hypothesis is that there are different subsets of riders which we may be able to identify based upon:
* pickup location
* dropoff location
* pickup area demographic attributes
* time of ride
* day of ride

We will use tree-based models to model tips as well as fares

```{r}
colsToUse=c("company","extras","fare","payment_type","tips","tolls","trip_miles","trip_seconds","trip_total","pickup_area","dropoff_area","Pop16Plus","IN_LBFRC","UNEMP","MedIncome","startDate","endDate","endDay","endMonth","endHour","LaborPart","pickup_area","dropoff_area")
#Drop any row containing NA in any column
dfTree <- df[,colsToUse]
dfTree <- dfTree[complete.cases(dfTree),]

set.seed(1)

nSample <- floor(0.75*nrow(dfTree))
train_ind <- sample(seq_len(nrow(dfTree)), size = nSample)

trainSample <- dfTree[train_ind,] #drop the final redundant dummy var for target_control
validateSample <- dfTree[-train_ind,]
```
We will construct a big tree to start.
```{r}
big_tree = rpart(tips~., data=trainSample, control=rpart.control(minsplit=100,cp=0.00001,xval=10))
nbig = length(unique(big_tree$where)) 

cat('size of big tree: ',nbig,'\n')

cptable = printcp(big_tree)
# this is the cp parameter with smallest cv-errror
(index_cp_min = which.min(cptable[,"xerror"]))
(cp_min = cptable[ index_cp_min, "CP" ])   

(val_h = cptable[index_cp_min, "xerror"] + cptable[index_cp_min, "xstd"])
(index_cp_std = Position(function(x) x < val_h, cptable[, "xerror"]))
(cp_std = cptable[ index_cp_std, "CP" ])   
```



We then prune back this big tree for a range of different CP, calculating the misclassification rate and loss as a function of tree size for both our training and validation data.
```{r}
cpvec = big_tree$cptable[,"CP"]
ntree = length(cpvec)
tree_size = rep(0,ntree)

errorTrain=rep(0,ntree)
errorValidate=rep(0,ntree)

for(i in 1:ntree) {
  temptree = prune(big_tree,cp=cpvec[i])
  tree_size[i] = length(unique(temptree$where))
 
  errorTrain[i]=sqrt(mean((predict(temptree,newdata=trainSample)-trainSample$tips)^2))
  errorValidate[i]=sqrt(mean((predict(temptree,newdata=validateSample)-validateSample$tips)^2))
}

ggplot()+geom_line(aes(x=tree_size,y=errorValidate, color="validate"),size=1) +  geom_line(aes(x=tree_size,y=errorTrain,color="train"))+  scale_colour_manual(breaks = c("Validate"="blue", "Train"="green"),values = c("blue", "green")) + ggtitle("Tree Performance")+xlab("Tree Size")+ylab("RMSE")

```

This indicates that the tip predictions from an 80-node tree are roughly ~$0.60 per trip.
```{r, "Specify train/validation data"}
#split sample into training and testing
n = nrow(dfSlim)
#train_index = sample(n, size = n * 0.9, replace = FALSE)
train_index = seq(1:floor(.9*n))
tdf = dfSlim[train_index,] #training_data
vdf = dfSlim[-train_index, ]#validation_data
```

##Logistic Regressions

```{r, "Logistic Regression - fare"}
#logistic regression, y=FALSE and model=FALSE to save memory
lgfit=glm(fare~.,tdf,family="gaussian",y=FALSE,model=FALSE)

phat = predict(lgfit, vdf, type="response")
```

